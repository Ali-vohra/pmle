{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c92ee326-bbbc-4f2d-a178-73c170123a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘ImageClassification’: File exists\n"
     ]
    }
   ],
   "source": [
    "! mkdir ImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d89a1f09-8bc3-4bf8-9b89-7b9322ad7a93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: tensorflow-serving-api 2.3.0 has a non-standard dependency specifier grpcio>=1.0<2. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of tensorflow-serving-api or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade --quiet {USER_FLAG} google-cloud-aiplatform kfp google-cloud-pipeline-components==1.0.40 google-cloud-storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20c376c2-6dd1-495e-a150-f35f6eddb960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f04a4c6-411d-4675-a323-24f7df586dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  qwiklabs-gcp-00-d51a197e88bc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "939a52dc-b15c-4a5a-ac89-d639d39200a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘pipelines’: File exists\n"
     ]
    }
   ],
   "source": [
    "! mkdir pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7a28538-1356-4468-902a-7e2e239c4d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGION:  us-central1\n"
     ]
    }
   ],
   "source": [
    "REGION=!curl -s -X GET \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/gcp_region\" -H \"Metadata-Flavor: Google\"\n",
    "REGION = REGION[-1]\n",
    "\n",
    "print(\"REGION: \", REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cfb2e5f-33f4-4fc6-8911-e326189fd0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-central1\n",
      "qwiklabs-gcp-00-d51a197e88bc-bucket\n",
      "qwiklabs-gcp-00-d51a197e88bc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "BUCKET=PROJECT_ID + \"-bucket\"\n",
    "\n",
    "# set environment variable for reference later.\n",
    "os.environ['REGION']=REGION\n",
    "print(os.getenv('REGION'))\n",
    "\n",
    "os.environ['BUCKET']=BUCKET\n",
    "print(os.getenv('BUCKET'))\n",
    "\n",
    "os.environ['PROJECT_ID']=PROJECT_ID\n",
    "print(os.getenv('PROJECT_ID'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f784986a-3137-4bdd-b144-3c7c6a91378b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "google-cloud-aiplatform\n",
    "kfp\n",
    "google-cloud-pipeline-components==1.0.40\n",
    "google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e144809-dc7c-42a8-884d-ee2bb4568033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pipelines/train_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pipelines/train_pipeline.py\n",
    "\n",
    "import os\n",
    "\n",
    "import kfp\n",
    "import time\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import component, pipeline, Artifact, ClassificationMetrics, Input, Output, Model, Metrics, Dataset\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from typing import NamedTuple, Dict\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "#Main pipeline class\n",
    "class pipeline_controller():\n",
    "    def __init__(self, template_path, display_name, pipeline_root, project_id, region):\n",
    "        self.template_path = template_path\n",
    "        self.display_name = display_name\n",
    "        self.pipeline_root = pipeline_root\n",
    "        self.project_id = project_id\n",
    "        self.region = region\n",
    "    \n",
    "    def _build_compile_pipeline(self):\n",
    "        \"\"\"Method to build and compile pipeline\"\"\"\n",
    "        self.pipeline = self._get_pipeline(self.project_id, self.region)\n",
    "        compiler.Compiler().compile(\n",
    "            pipeline_func=self.pipeline, package_path=self.template_path\n",
    "        )\n",
    "        \n",
    "    def _submit_job(self):\n",
    "        \"\"\"Method to Submit ML Pipeline job\"\"\"\n",
    "        #Next, define the job:\n",
    "        ml_pipeline_job = aiplatform.PipelineJob(\n",
    "            display_name=self.display_name,\n",
    "            template_path=self.template_path,\n",
    "            pipeline_root=self.pipeline_root,\n",
    "            project=self.project_id,\n",
    "            location=self.region,\n",
    "            # parameter_values={\"project\": self.project_id, \"display_name\": self.display_name},\n",
    "            enable_caching=False\n",
    "        )\n",
    "\n",
    "        #And finally, run the job:\n",
    "        ml_pipeline_job.submit()\n",
    "    \n",
    "    def _get_pipeline(self, PROJECT_ID, REGION):\n",
    "        ## Light weight component to create an Image DS\n",
    "        @component(\n",
    "            base_image=\"python:3.9-slim\",\n",
    "            packages_to_install=[\"google-api-core==2.10.2\", \"google-cloud\", \"google-cloud-aiplatform\", \"typing\", \"kfp\"],\n",
    "        )\n",
    "        def create_ds(project: str, \n",
    "                      display_name: str, \n",
    "                      gcs_source: str, \n",
    "                      import_schema_uri: str, \n",
    "                      timeout: int, \n",
    "                      dataset: Output[Dataset]):\n",
    "\n",
    "            from google.cloud import aiplatform\n",
    "            from google.cloud.aiplatform import datasets\n",
    "            from kfp.v2.dsl import Dataset\n",
    "\n",
    "            aiplatform.init(project=project)\n",
    "\n",
    "            obj_dataset = datasets.ImageDataset.create(\n",
    "                display_name=display_name,\n",
    "                gcs_source=gcs_source,\n",
    "                import_schema_uri=import_schema_uri,\n",
    "                create_request_timeout=timeout,\n",
    "            )\n",
    "\n",
    "            obj_dataset.wait()\n",
    "\n",
    "            dataset.uri = obj_dataset.gca_resource.name\n",
    "            dataset.metadata = {\n",
    "                'resourceName': obj_dataset.gca_resource.name\n",
    "            }\n",
    "        \n",
    "        \"\"\"Main method to Create pipeline\"\"\"\n",
    "        @pipeline(name=self.display_name,\n",
    "                    pipeline_root=self.pipeline_root)\n",
    "        def pipeline_fn(\n",
    "            project: str = PROJECT_ID, \n",
    "            region: str = REGION\n",
    "        ):\n",
    "            \n",
    "            from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "            from google_cloud_pipeline_components.v1.endpoint import (EndpointCreateOp, ModelDeployOp)\n",
    "            import google.cloud.aiplatform as aip\n",
    "\n",
    "            ds_op = create_ds(\n",
    "                project=project,\n",
    "                display_name=\"flowers\",\n",
    "                gcs_source=\"gs://cloud-samples-data/vision/automl_classification/flowers/all_data_v2.csv\",\n",
    "                import_schema_uri=aip.schema.dataset.ioformat.image.single_label_classification,\n",
    "                timeout=3600\n",
    "            )\n",
    "\n",
    "            training_job_run_op = gcc_aip.AutoMLImageTrainingJobRunOp(\n",
    "                project=project,\n",
    "                location=region,\n",
    "                display_name=\"train-automl-flowers\",\n",
    "                prediction_type=\"classification\",\n",
    "                model_type=\"CLOUD\",\n",
    "                dataset=ds_op.outputs[\"dataset\"].ignore_type(),\n",
    "                model_display_name=\"train-automl-flowers\",\n",
    "                training_fraction_split=0.6,\n",
    "                validation_fraction_split=0.2,\n",
    "                test_fraction_split=0.2,\n",
    "                budget_milli_node_hours=8000,\n",
    "            )\n",
    "\n",
    "            endpoint_op = EndpointCreateOp(\n",
    "                project=project,\n",
    "                location=region,\n",
    "                display_name=\"train-automl-flowers\",\n",
    "            )\n",
    "\n",
    "            ModelDeployOp(\n",
    "                model=training_job_run_op.outputs[\"model\"],\n",
    "                endpoint=endpoint_op.outputs[\"endpoint\"],\n",
    "                automatic_resources_min_replica_count=1,\n",
    "                automatic_resources_max_replica_count=1,\n",
    "            )\n",
    "\n",
    "        #Returns as the output of _get_pipeline()\n",
    "        return pipeline_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c71f7582-c3dd-4849-af18-b64f289d48a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGION: us-central1\n",
      "PROJECT_ID: qwiklabs-gcp-00-d51a197e88bc\n",
      "BUCKET: qwiklabs-gcp-00-d51a197e88bc-bucket\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "BUCKET=PROJECT_ID + \"-bucket\"\n",
    "\n",
    "# set environment variable for reference later.\n",
    "os.environ['REGION']=REGION\n",
    "print(\"REGION: \" + os.getenv('REGION'))\n",
    "\n",
    "os.environ['PROJECT_ID']=PROJECT_ID\n",
    "print(\"PROJECT_ID: \" + os.getenv('PROJECT_ID'))\n",
    "\n",
    "os.environ['BUCKET']=BUCKET\n",
    "print(\"BUCKET: \" + os.getenv('BUCKET'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "452b7be0-8b1a-4bf7-8431-6abd9bd22482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting build_and_deploy.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile build_and_deploy.py\n",
    "\n",
    "###Code to Build and Deploy the full pipeline\n",
    "###This will be used in Cloud Builder\n",
    "\n",
    "#Initialize pipeline object\n",
    "from pipelines.train_pipeline import pipeline_controller\n",
    "import time\n",
    "import os\n",
    "\n",
    "REGION=\"us-central1\"\n",
    "PROJECT_ID=\"qwiklabs-gcp-00-d51a197e88bc\"\n",
    "BUCKET=f\"{PROJECT_ID}-bucket\"\n",
    "\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET}/pipeline_root/\"\n",
    "DISPLAY_NAME = 'vertex-customml-pipeline{}'.format(str(int(time.time())))\n",
    "\n",
    "print(\"Building pipeline {}\".format(DISPLAY_NAME))\n",
    "\n",
    "pipe = pipeline_controller(template_path=\"pipeline.json\",\n",
    "                           display_name=\"vertex-automlimage-classif\", \n",
    "                           pipeline_root=PIPELINE_ROOT,\n",
    "                           project_id=PROJECT_ID,\n",
    "                           region=REGION)\n",
    "\n",
    "#Build and Compile pipeline\n",
    "pipe._build_compile_pipeline()\n",
    "\n",
    "##Submit Job\n",
    "pipe._submit_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7634c2b9-ef2e-4f30-9b84-04914efbb516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building pipeline vertex-customml-pipeline1706892541\n",
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n",
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/305160138672/locations/us-central1/pipelineJobs/vertex-automlimage-classif-20240202164901\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/305160138672/locations/us-central1/pipelineJobs/vertex-automlimage-classif-20240202164901')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/vertex-automlimage-classif-20240202164901?project=305160138672\n"
     ]
    }
   ],
   "source": [
    "!python -m build_and_deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd0dc884-3870-48f4-89cf-5f411c235406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "# Download dependencies to /root/.local.\n",
    "FROM python:3.7-slim AS builder\n",
    "COPY requirements.txt .\n",
    "\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "COPY . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "278f9ade-bea9-48fb-bf1f-688de3bca3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists\n"
     ]
    }
   ],
   "source": [
    "!gcloud artifacts repositories create vertex-gar \\\n",
    "    --project=$PROJECT_ID \\\n",
    "    --repository-format=Docker \\\n",
    "    --location=$REGION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f60cec43-49e6-4855-88a7-aaf75c85d1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile cloudbuild.yaml\n",
    "\n",
    "# Create config file and build a new image tagged with the given commit hash.\n",
    "\n",
    "steps:\n",
    "##Step 1 -> Build main image from Dockerfile\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  id: 'build_image'\n",
    "  args: [\n",
    "    'build', '-t', '{REGION}-docker.pkg.dev/{PROJECT_ID}/vertex-gar/latest:latest',\n",
    "    '-f', 'Dockerfile', '.',\n",
    "  ]\n",
    "\n",
    "##Step 2 -> Deploy Docker image to Artifact Repository\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  id: 'push_image'\n",
    "  waitFor:\n",
    "    - 'build_image'\n",
    "  args: [\n",
    "    'push', '{REGION}-docker.pkg.dev/{PROJECT_ID}/vertex-gar/latest:latest'\n",
    "  ]\n",
    "\n",
    "##Step 3 -> Deploy pipeline to Vertex AI( using above built image )\n",
    "- name: '{REGION}-docker.pkg.dev/{PROJECT_ID}/vertex-gar/latest:latest'\n",
    "  id: 'deploy_pipelines'\n",
    "  waitFor:\n",
    "    - 'push_image'\n",
    "  entrypoint: /bin/bash\n",
    "  args:\n",
    "    - -c\n",
    "    - |\n",
    "      python -m build_and_deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd1b82a4-cc76-4061-89e0-e647997acb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the following command to replace {PROJECT_ID}/{REGION} with project id/region\n",
    "cmd_str_proj = 's/[{]PROJECT_ID[}]/' + PROJECT_ID + '/g'\n",
    "cmd_str_region = 's/[{]REGION[}]/' + REGION + '/g'\n",
    "! sed -i $cmd_str_proj cloudbuild.yaml\n",
    "! sed -i $cmd_str_region cloudbuild.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b08866d-de5c-4b83-bd2c-f06a6e37ef1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 8259 file(s) totalling 240.1 MiB before compression.\n",
      "Uploading tarball of [.] to [gs://qwiklabs-gcp-00-d51a197e88bc_cloudbuild/source/1706893087.206182-e1853222bd0f49db85ba25a0c2bd3654.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-gcp-00-d51a197e88bc/locations/global/builds/6aea308b-ff66-42d0-9b03-7b1afe1b2cca].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/6aea308b-ff66-42d0-9b03-7b1afe1b2cca?project=305160138672 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"6aea308b-ff66-42d0-9b03-7b1afe1b2cca\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-gcp-00-d51a197e88bc_cloudbuild/source/1706893087.206182-e1853222bd0f49db85ba25a0c2bd3654.tgz#1706893134579331\n",
      "Copying gs://qwiklabs-gcp-00-d51a197e88bc_cloudbuild/source/1706893087.206182-e1853222bd0f49db85ba25a0c2bd3654.tgz#1706893134579331...\n",
      "| [1 files][142.1 MiB/142.1 MiB]                                                \n",
      "Operation completed over 1 objects/142.1 MiB.\n",
      "BUILD\n",
      "Starting Step #0 - \"build_image\"\n",
      "Step #0 - \"build_image\": Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #0 - \"build_image\": Sending build context to Docker daemon    258MB\n",
      "Step #0 - \"build_image\": Step 1/4 : FROM python:3.7-slim AS builder\n",
      "Step #0 - \"build_image\": 3.7-slim: Pulling from library/python\n",
      "Step #0 - \"build_image\": a803e7c4b030: Pulling fs layer\n",
      "Step #0 - \"build_image\": bf3336e84c8e: Pulling fs layer\n",
      "Step #0 - \"build_image\": 8973eb85275f: Pulling fs layer\n",
      "Step #0 - \"build_image\": f9afc3cc0135: Pulling fs layer\n",
      "Step #0 - \"build_image\": 39312d8b4ab7: Pulling fs layer\n",
      "Step #0 - \"build_image\": f9afc3cc0135: Waiting\n",
      "Step #0 - \"build_image\": 39312d8b4ab7: Waiting\n",
      "Step #0 - \"build_image\": bf3336e84c8e: Verifying Checksum\n",
      "Step #0 - \"build_image\": bf3336e84c8e: Download complete\n",
      "Step #0 - \"build_image\": a803e7c4b030: Verifying Checksum\n",
      "Step #0 - \"build_image\": a803e7c4b030: Download complete\n",
      "Step #0 - \"build_image\": 8973eb85275f: Verifying Checksum\n",
      "Step #0 - \"build_image\": 8973eb85275f: Download complete\n",
      "Step #0 - \"build_image\": f9afc3cc0135: Verifying Checksum\n",
      "Step #0 - \"build_image\": f9afc3cc0135: Download complete\n",
      "Step #0 - \"build_image\": 39312d8b4ab7: Verifying Checksum\n",
      "Step #0 - \"build_image\": 39312d8b4ab7: Download complete\n",
      "Step #0 - \"build_image\": a803e7c4b030: Pull complete\n",
      "Step #0 - \"build_image\": bf3336e84c8e: Pull complete\n",
      "Step #0 - \"build_image\": 8973eb85275f: Pull complete\n",
      "Step #0 - \"build_image\": f9afc3cc0135: Pull complete\n",
      "Step #0 - \"build_image\": 39312d8b4ab7: Pull complete\n",
      "Step #0 - \"build_image\": Digest: sha256:b53f496ca43e5af6994f8e316cf03af31050bf7944e0e4a308ad86c001cf028b\n",
      "Step #0 - \"build_image\": Status: Downloaded newer image for python:3.7-slim\n",
      "Step #0 - \"build_image\":  ---> a255ffcb469f\n",
      "Step #0 - \"build_image\": Step 2/4 : COPY requirements.txt .\n",
      "Step #0 - \"build_image\":  ---> 6180aefa7ebe\n",
      "Step #0 - \"build_image\": Step 3/4 : RUN pip install -r requirements.txt\n",
      "Step #0 - \"build_image\":  ---> Running in 917d490766b4\n",
      "Step #0 - \"build_image\": Collecting google-cloud-aiplatform\n",
      "Step #0 - \"build_image\":   Downloading google_cloud_aiplatform-1.34.0-py2.py3-none-any.whl (3.1 MB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 19.6 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting kfp\n",
      "Step #0 - \"build_image\":   Downloading kfp-2.6.0.tar.gz (425 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 425.3/425.3 kB 25.7 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\":   Preparing metadata (setup.py): started\n",
      "Step #0 - \"build_image\":   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0 - \"build_image\": Collecting google-cloud-pipeline-components==1.0.40\n",
      "Step #0 - \"build_image\":   Downloading google_cloud_pipeline_components-1.0.40-py3-none-any.whl (1.0 MB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 31.2 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting google-cloud-storage\n",
      "Step #0 - \"build_image\":   Downloading google_cloud_storage-2.14.0-py2.py3-none-any.whl (121 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.6/121.6 kB 15.6 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting kfp\n",
      "Step #0 - \"build_image\":   Downloading kfp-1.8.22.tar.gz (304 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 304.9/304.9 kB 30.5 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\":   Preparing metadata (setup.py): started\n",
      "Step #0 - \"build_image\":   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0 - \"build_image\": Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.31.5\n",
      "Step #0 - \"build_image\":   Downloading google_api_core-2.16.1-py3-none-any.whl (135 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.2/135.2 kB 18.3 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting grpcio-status<=1.47.0\n",
      "Step #0 - \"build_image\":   Downloading grpcio_status-1.47.0-py3-none-any.whl (10.0 kB)\n",
      "Step #0 - \"build_image\": Collecting protobuf<4.0.0dev,>=3.19.0\n",
      "Step #0 - \"build_image\":   Downloading protobuf-3.20.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 33.7 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting google-cloud-notebooks>=0.4.0\n",
      "Step #0 - \"build_image\":   Downloading google_cloud_notebooks-1.10.0-py2.py3-none-any.whl (267 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 267.9/267.9 kB 32.2 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "Step #0 - \"build_image\":   Downloading googleapis_common_protos-1.62.0-py2.py3-none-any.whl (228 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 228.7/228.7 kB 30.5 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "Step #0 - \"build_image\":   Downloading google_cloud_resource_manager-1.12.0-py2.py3-none-any.whl (333 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 333.9/333.9 kB 35.5 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting shapely<2.0.0\n",
      "Step #0 - \"build_image\":   Downloading Shapely-1.8.5.post1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.0 MB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 42.8 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting proto-plus<2.0.0dev,>=1.22.0\n",
      "Step #0 - \"build_image\":   Downloading proto_plus-1.23.0-py3-none-any.whl (48 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.8/48.8 kB 7.1 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting packaging>=14.3\n",
      "Step #0 - \"build_image\":   Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 8.5 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting google-cloud-bigquery<4.0.0dev,>=1.15.0\n",
      "Step #0 - \"build_image\":   Downloading google_cloud_bigquery-3.17.1-py2.py3-none-any.whl (230 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 230.2/230.2 kB 26.9 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting absl-py<2,>=0.9\n",
      "Step #0 - \"build_image\":   Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 19.9 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting PyYAML<7,>=5.3\n",
      "Step #0 - \"build_image\":   Downloading PyYAML-6.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (670 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 670.1/670.1 kB 40.2 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting kubernetes<26,>=8.0.0\n",
      "Step #0 - \"build_image\":   Downloading kubernetes-25.3.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 40.3 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting google-api-python-client<2,>=1.7.8\n",
      "Step #0 - \"build_image\":   Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.1/62.1 kB 10.1 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting google-auth<3,>=1.6.1\n",
      "Step #0 - \"build_image\":   Downloading google_auth-2.27.0-py2.py3-none-any.whl (186 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 186.8/186.8 kB 24.2 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting requests-toolbelt<1,>=0.8.0\n",
      "Step #0 - \"build_image\":   Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.5/54.5 kB 8.9 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting cloudpickle<3,>=2.0.0\n",
      "Step #0 - \"build_image\":   Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Step #0 - \"build_image\": Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "Step #0 - \"build_image\":   Downloading kfp-server-api-1.8.5.tar.gz (58 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.1/58.1 kB 9.5 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\":   Preparing metadata (setup.py): started\n",
      "Step #0 - \"build_image\":   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0 - \"build_image\": Collecting jsonschema<5,>=3.0.1\n",
      "Step #0 - \"build_image\":   Downloading jsonschema-4.17.3-py3-none-any.whl (90 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90.4/90.4 kB 15.2 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting tabulate<1,>=0.8.6\n",
      "Step #0 - \"build_image\":   Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Step #0 - \"build_image\": Collecting click<9,>=7.1.2\n",
      "Step #0 - \"build_image\":   Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 15.1 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting Deprecated<2,>=1.2.7\n",
      "Step #0 - \"build_image\":   Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Step #0 - \"build_image\": Collecting strip-hints<1,>=0.1.8\n",
      "Step #0 - \"build_image\":   Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "Step #0 - \"build_image\":   Preparing metadata (setup.py): started\n",
      "Step #0 - \"build_image\":   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0 - \"build_image\": Collecting docstring-parser<1,>=0.7.3\n",
      "Step #0 - \"build_image\":   Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
      "Step #0 - \"build_image\": Collecting kfp-pipeline-spec<0.2.0,>=0.1.16\n",
      "Step #0 - \"build_image\":   Downloading kfp_pipeline_spec-0.1.16-py3-none-any.whl (19 kB)\n",
      "Step #0 - \"build_image\": Collecting fire<1,>=0.3.1\n",
      "Step #0 - \"build_image\":   Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.3/88.3 kB 14.1 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\":   Preparing metadata (setup.py): started\n",
      "Step #0 - \"build_image\":   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0 - \"build_image\": Collecting uritemplate<4,>=3.0.1\n",
      "Step #0 - \"build_image\":   Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Step #0 - \"build_image\": Collecting urllib3<2\n",
      "Step #0 - \"build_image\":   Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143.8/143.8 kB 21.4 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting pydantic<2,>=1.8.2\n",
      "Step #0 - \"build_image\":   Downloading pydantic-1.10.14-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 58.6 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting typer<1.0,>=0.3.2\n",
      "Step #0 - \"build_image\":   Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.9/45.9 kB 6.9 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting typing-extensions<5,>=3.7.4\n",
      "Step #0 - \"build_image\":   Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Step #0 - \"build_image\": Collecting google-cloud-core<3.0dev,>=2.3.0\n",
      "Step #0 - \"build_image\":   Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\n",
      "Step #0 - \"build_image\": Collecting google-crc32c<2.0dev,>=1.0\n",
      "Step #0 - \"build_image\":   Downloading google_crc32c-1.5.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Step #0 - \"build_image\": Collecting google-resumable-media>=2.6.0\n",
      "Step #0 - \"build_image\":   Downloading google_resumable_media-2.7.0-py2.py3-none-any.whl (80 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.6/80.6 kB 11.9 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting requests<3.0.0dev,>=2.18.0\n",
      "Step #0 - \"build_image\":   Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.6/62.6 kB 9.4 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting importlib-metadata\n",
      "Step #0 - \"build_image\":   Downloading importlib_metadata-6.7.0-py3-none-any.whl (22 kB)\n",
      "Step #0 - \"build_image\": Collecting wrapt<2,>=1.10\n",
      "Step #0 - \"build_image\":   Downloading wrapt-1.16.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.5/77.5 kB 13.2 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting six\n",
      "Step #0 - \"build_image\":   Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Step #0 - \"build_image\": Collecting termcolor\n",
      "Step #0 - \"build_image\":   Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Step #0 - \"build_image\": Collecting grpcio<2.0dev,>=1.33.2\n",
      "Step #0 - \"build_image\":   Downloading grpcio-1.60.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/5.4 MB 56.9 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting google-auth-httplib2>=0.0.3\n",
      "Step #0 - \"build_image\":   Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Step #0 - \"build_image\": Collecting httplib2<1dev,>=0.15.0\n",
      "Step #0 - \"build_image\":   Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.9/96.9 kB 13.8 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting cachetools<6.0,>=2.0.0\n",
      "Step #0 - \"build_image\":   Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Step #0 - \"build_image\": Collecting rsa<5,>=3.1.4\n",
      "Step #0 - \"build_image\":   Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Step #0 - \"build_image\": Collecting pyasn1-modules>=0.2.1\n",
      "Step #0 - \"build_image\":   Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.3/181.3 kB 24.2 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting python-dateutil<3.0dev,>=2.7.2\n",
      "Step #0 - \"build_image\":   Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 kB 25.6 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4\n",
      "Step #0 - \"build_image\":   Downloading grpc_google_iam_v1-0.13.0-py2.py3-none-any.whl (25 kB)\n",
      "Step #0 - \"build_image\": Collecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0\n",
      "Step #0 - \"build_image\":   Downloading pyrsistent-0.19.3-py3-none-any.whl (57 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 8.8 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting importlib-resources>=1.4.0\n",
      "Step #0 - \"build_image\":   Downloading importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
      "Step #0 - \"build_image\": Collecting pkgutil-resolve-name>=1.3.10\n",
      "Step #0 - \"build_image\":   Downloading pkgutil_resolve_name-1.3.10-py3-none-any.whl (4.7 kB)\n",
      "Step #0 - \"build_image\": Collecting attrs>=17.4.0\n",
      "Step #0 - \"build_image\":   Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.8/60.8 kB 9.7 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting certifi\n",
      "Step #0 - \"build_image\":   Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 163.8/163.8 kB 19.6 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting requests-oauthlib\n",
      "Step #0 - \"build_image\":   Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Step #0 - \"build_image\": Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0\n",
      "Step #0 - \"build_image\":   Downloading websocket_client-1.6.1-py3-none-any.whl (56 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.9/56.9 kB 8.2 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.7/site-packages (from kubernetes<26,>=8.0.0->kfp->-r requirements.txt (line 2)) (57.5.0)\n",
      "Step #0 - \"build_image\": Collecting charset-normalizer<4,>=2\n",
      "Step #0 - \"build_image\":   Downloading charset_normalizer-3.3.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 136.8/136.8 kB 20.6 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting idna<4,>=2.5\n",
      "Step #0 - \"build_image\":   Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.6/61.6 kB 10.5 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Requirement already satisfied: wheel in /usr/local/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp->-r requirements.txt (line 2)) (0.41.2)\n",
      "Step #0 - \"build_image\": Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2\n",
      "Step #0 - \"build_image\":   Downloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103.1/103.1 kB 16.0 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting zipp>=0.5\n",
      "Step #0 - \"build_image\":   Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Step #0 - \"build_image\": Collecting pyasn1<0.6.0,>=0.4.6\n",
      "Step #0 - \"build_image\":   Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.9/84.9 kB 14.0 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Collecting oauthlib>=3.0.0\n",
      "Step #0 - \"build_image\":   Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Step #0 - \"build_image\":      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 21.9 MB/s eta 0:00:00\n",
      "Step #0 - \"build_image\": Building wheels for collected packages: kfp, fire, kfp-server-api, strip-hints\n",
      "Step #0 - \"build_image\":   Building wheel for kfp (setup.py): started\n",
      "Step #0 - \"build_image\":   Building wheel for kfp (setup.py): finished with status 'done'\n",
      "Step #0 - \"build_image\":   Created wheel for kfp: filename=kfp-1.8.22-py3-none-any.whl size=426989 sha256=bcb029095abb705cf2f2b8e5e1420223da0aa8c7b51decfa2ef3dd4721198883\n",
      "Step #0 - \"build_image\":   Stored in directory: /root/.cache/pip/wheels/f1/ea/2f/99e16eb441be7831b69cb568df54a0dd04ab31cb758bfc6819\n",
      "Step #0 - \"build_image\":   Building wheel for fire (setup.py): started\n",
      "Step #0 - \"build_image\":   Building wheel for fire (setup.py): finished with status 'done'\n",
      "Step #0 - \"build_image\":   Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116950 sha256=b2748924ff94ad2b863ab757fa8ac082b52975ce8fb0ef172f1741a8536d7a9f\n",
      "Step #0 - \"build_image\":   Stored in directory: /root/.cache/pip/wheels/20/97/e1/dd2c472bebcdcaa85fdc07d0f19020299f1c86773028860c53\n",
      "Step #0 - \"build_image\":   Building wheel for kfp-server-api (setup.py): started\n",
      "Step #0 - \"build_image\":   Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "Step #0 - \"build_image\":   Created wheel for kfp-server-api: filename=kfp_server_api-1.8.5-py3-none-any.whl size=99710 sha256=7c96a772d8b68522b2310fa6b64da0a4f637c21b75a427090008057a5417f597\n",
      "Step #0 - \"build_image\":   Stored in directory: /root/.cache/pip/wheels/77/0e/7b/ed385d69453b7b754834c01d83fa9f5708ba66b4f6ed5d6a35\n",
      "Step #0 - \"build_image\":   Building wheel for strip-hints (setup.py): started\n",
      "Step #0 - \"build_image\":   Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "Step #0 - \"build_image\":   Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22300 sha256=a5fe649c7f557b397c1f2d4ab959d96716fdfff7478222d03888d793af0ba555\n",
      "Step #0 - \"build_image\":   Stored in directory: /root/.cache/pip/wheels/5e/14/c3/6e44e9b2545f2d570b03f5b6d38c00b7534aa8abb376978363\n",
      "Step #0 - \"build_image\": Successfully built kfp fire kfp-server-api strip-hints\n",
      "Step #0 - \"build_image\": Installing collected packages: zipp, wrapt, websocket-client, urllib3, uritemplate, typing-extensions, termcolor, tabulate, strip-hints, six, shapely, PyYAML, pyrsistent, pyparsing, pyasn1, protobuf, pkgutil-resolve-name, packaging, oauthlib, idna, grpcio, google-crc32c, docstring-parser, cloudpickle, charset-normalizer, certifi, cachetools, absl-py, rsa, requests, python-dateutil, pydantic, pyasn1-modules, proto-plus, kfp-pipeline-spec, importlib-resources, importlib-metadata, httplib2, googleapis-common-protos, google-resumable-media, fire, Deprecated, requests-toolbelt, requests-oauthlib, kfp-server-api, grpcio-status, google-auth, click, attrs, typer, kubernetes, jsonschema, grpc-google-iam-v1, google-auth-httplib2, google-api-core, google-cloud-core, google-api-python-client, google-cloud-storage, google-cloud-resource-manager, google-cloud-notebooks, google-cloud-bigquery, kfp, google-cloud-aiplatform, google-cloud-pipeline-components\n",
      "Step #0 - \"build_image\": Successfully installed Deprecated-1.2.14 PyYAML-6.0.1 absl-py-1.4.0 attrs-23.2.0 cachetools-5.3.2 certifi-2024.2.2 charset-normalizer-3.3.2 click-8.1.7 cloudpickle-2.2.1 docstring-parser-0.15 fire-0.5.0 google-api-core-2.16.1 google-api-python-client-1.12.11 google-auth-2.27.0 google-auth-httplib2-0.2.0 google-cloud-aiplatform-1.34.0 google-cloud-bigquery-3.17.1 google-cloud-core-2.4.1 google-cloud-notebooks-1.10.0 google-cloud-pipeline-components-1.0.40 google-cloud-resource-manager-1.12.0 google-cloud-storage-2.14.0 google-crc32c-1.5.0 google-resumable-media-2.7.0 googleapis-common-protos-1.62.0 grpc-google-iam-v1-0.13.0 grpcio-1.60.1 grpcio-status-1.47.0 httplib2-0.22.0 idna-3.6 importlib-metadata-6.7.0 importlib-resources-5.12.0 jsonschema-4.17.3 kfp-1.8.22 kfp-pipeline-spec-0.1.16 kfp-server-api-1.8.5 kubernetes-25.3.0 oauthlib-3.2.2 packaging-23.2 pkgutil-resolve-name-1.3.10 proto-plus-1.23.0 protobuf-3.20.3 pyasn1-0.5.1 pyasn1-modules-0.3.0 pydantic-1.10.14 pyparsing-3.1.1 pyrsistent-0.19.3 python-dateutil-2.8.2 requests-2.31.0 requests-oauthlib-1.3.1 requests-toolbelt-0.10.1 rsa-4.9 shapely-1.8.5.post1 six-1.16.0 strip-hints-0.1.10 tabulate-0.9.0 termcolor-2.3.0 typer-0.9.0 typing-extensions-4.7.1 uritemplate-3.0.1 urllib3-1.26.18 websocket-client-1.6.1 wrapt-1.16.0 zipp-3.15.0\n",
      "Step #0 - \"build_image\": \u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Step #0 - \"build_image\": \u001b[0m\u001b[91m\n",
      "Step #0 - \"build_image\": [notice] A new release of pip is available: 23.0.1 -> 23.3.2\n",
      "Step #0 - \"build_image\": [notice] To update, run: pip install --upgrade pip\n",
      "Step #0 - \"build_image\": \u001b[0mRemoving intermediate container 917d490766b4\n",
      "Step #0 - \"build_image\":  ---> d8e042cc1789\n",
      "Step #0 - \"build_image\": Step 4/4 : COPY . .\n",
      "Step #0 - \"build_image\":  ---> 2b0af3be21b8\n",
      "Step #0 - \"build_image\": Successfully built 2b0af3be21b8\n",
      "Step #0 - \"build_image\": Successfully tagged us-central1-docker.pkg.dev/qwiklabs-gcp-00-d51a197e88bc/vertex-gar/latest:latest\n",
      "Finished Step #0 - \"build_image\"\n",
      "Starting Step #1 - \"push_image\"\n",
      "Step #1 - \"push_image\": Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #1 - \"push_image\": The push refers to repository [us-central1-docker.pkg.dev/qwiklabs-gcp-00-d51a197e88bc/vertex-gar/latest]\n",
      "Step #1 - \"push_image\": 5d3f813af85a: Preparing\n",
      "Step #1 - \"push_image\": 13db956cd026: Preparing\n",
      "Step #1 - \"push_image\": cc4ea382dc60: Preparing\n",
      "Step #1 - \"push_image\": b8594deafbe5: Preparing\n",
      "Step #1 - \"push_image\": 8a55150afecc: Preparing\n",
      "Step #1 - \"push_image\": ad34ffec41dd: Preparing\n",
      "Step #1 - \"push_image\": f19cb1e4112d: Preparing\n",
      "Step #1 - \"push_image\": d310e774110a: Preparing\n",
      "Step #1 - \"push_image\": ad34ffec41dd: Waiting\n",
      "Step #1 - \"push_image\": f19cb1e4112d: Waiting\n",
      "Step #1 - \"push_image\": d310e774110a: Waiting\n",
      "Step #1 - \"push_image\": 8a55150afecc: Layer already exists\n",
      "Step #1 - \"push_image\": b8594deafbe5: Layer already exists\n",
      "Step #1 - \"push_image\": ad34ffec41dd: Layer already exists\n",
      "Step #1 - \"push_image\": f19cb1e4112d: Layer already exists\n",
      "Step #1 - \"push_image\": d310e774110a: Layer already exists\n",
      "Step #1 - \"push_image\": cc4ea382dc60: Pushed\n",
      "Step #1 - \"push_image\": 13db956cd026: Pushed\n",
      "Step #1 - \"push_image\": 5d3f813af85a: Pushed\n",
      "Step #1 - \"push_image\": latest: digest: sha256:4405dfb70dcb3357d8a3db9a15e71dce5ff3c8b51927314f0f3f4c7c420bfc82 size: 2002\n",
      "Finished Step #1 - \"push_image\"\n",
      "Starting Step #2 - \"deploy_pipelines\"\n",
      "Step #2 - \"deploy_pipelines\": Already have image (with digest): us-central1-docker.pkg.dev/qwiklabs-gcp-00-d51a197e88bc/vertex-gar/latest:latest\n",
      "Step #2 - \"deploy_pipelines\": Building pipeline vertex-customml-pipeline1706893236\n",
      "Step #2 - \"deploy_pipelines\": Creating PipelineJob\n",
      "Step #2 - \"deploy_pipelines\": PipelineJob created. Resource name: projects/305160138672/locations/us-central1/pipelineJobs/vertex-automlimage-classif-20240202170037\n",
      "Step #2 - \"deploy_pipelines\": To use this PipelineJob in another session:\n",
      "Step #2 - \"deploy_pipelines\": pipeline_job = aiplatform.PipelineJob.get('projects/305160138672/locations/us-central1/pipelineJobs/vertex-automlimage-classif-20240202170037')\n",
      "Step #2 - \"deploy_pipelines\": View Pipeline Job:\n",
      "Step #2 - \"deploy_pipelines\": https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/vertex-automlimage-classif-20240202170037?project=305160138672\n",
      "Step #2 - \"deploy_pipelines\": /usr/local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "Step #2 - \"deploy_pipelines\":   category=FutureWarning,\n",
      "Finished Step #2 - \"deploy_pipelines\"\n",
      "PUSH\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES  STATUS\n",
      "6aea308b-ff66-42d0-9b03-7b1afe1b2cca  2024-02-02T16:58:54+00:00  1M44S     gs://qwiklabs-gcp-00-d51a197e88bc_cloudbuild/source/1706893087.206182-e1853222bd0f49db85ba25a0c2bd3654.tgz  -       SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --config=cloudbuild.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3d6736-8f9c-4111-9f15-901b3e6a68ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-3.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
